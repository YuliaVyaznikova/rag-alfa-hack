{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3792bb5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "i:\\NSU\\alphafuture\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "# from langchain. import RecursiveCharacterTextSplitter\n",
    "from transformers import pipeline\n",
    "import faiss\n",
    "import numpy as np\n",
    "import ollama\n",
    "import PyPDF2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db4fa694",
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_pdf(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        text = \"\"\n",
    "        for page in pdf_reader.pages:\n",
    "            text += page.extract_text() or \"\"\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "859f5ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = from_pdf('./doc.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "368a0cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunks = [\n",
    "#     \"RAG — это Retrieval-Augmented Generation, метод, который сочетает генерацию текста с поиском релевантных данных.\",\n",
    "#     \"Модель сначала ищет релевантные документы по эмбеддингам, а потом использует их для генерации ответа.\",\n",
    "#     \"RAG особенно полезен, когда модель не может помнить все факты, и их можно искать в базе знаний.\",\n",
    "#     \"RAG обычно использует FAISS или похожие библиотеки для векторного поиска.\"\n",
    "# ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0285a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_chunk_text(text, chunk_size=200, overlap=50):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(words):\n",
    "        end = start + chunk_size\n",
    "        chunk = \" \".join(words[start:end])\n",
    "        chunks.append(chunk)\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "chunks = smart_chunk_text(doc, chunk_size=200, overlap=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "614e055d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106\n",
      "Preprint CONTINUOUSAUTOREGRESSIVELANGUAGEMODELS Chenze Shao1, Darren Li1,2, Fandong Meng1∗, Jie Zhou1 1WeChat AI, Tencent Inc2Qiuzhen College, Tsinghua University ABSTRACT The efficiency of large language models (LLMs) is fundamentally limited by their sequential, token-by-token generation process. We argue that overcoming this bottleneck requires a new design axis for LLM scaling: increasing the semantic bandwidth of each generative step. To this end, we introduce Continuous Autore- gressive Language Models (CALM), a paradigm shift from discrete next-token prediction to continuous next-vector prediction. CALM uses a high-fidelity au- toencoder to compress a chunk of K tokens into a single continuous vector, from which the original tokens can be reconstructed with over 99.9% accuracy. This allows us to model language as a sequence of continuous vectors instead of dis- crete tokens, which reduces the number of generative steps by a factor of K. The paradigm shift necessitates a new modeling toolkit; therefore, we develop a com- prehensive likelihood-free framework that enables robust training, evaluation, and controllable sampling in the continuous domain. Experiments show that CALM significantly improves the performance-compute trade-off, achieving the perfor- mance of strong discrete baselines at a significantly lower computational cost. More importantly, these findings establish next-vector prediction as a powerful and scalable\n",
      "prehensive likelihood-free framework that enables robust training, evaluation, and controllable sampling in the continuous domain. Experiments show that CALM significantly improves the performance-compute trade-off, achieving the perfor- mance of strong discrete baselines at a significantly lower computational cost. More importantly, these findings establish next-vector prediction as a powerful and scalable pathway towards ultra-efficient language models. Code:https://github.com/shaochenze/calm Project:https://shaochenze.github.io/blog/2025/CALM 1 INTRODUCTION Large Language Models (LLMs) have revolutionized the field of artificial intelligence, demonstrat- ing unprecedented capabilities in understanding, generating, and reasoning with human language (Achiam et al., 2023; Google, 2025; DeepSeek-AI, 2025). However, this remarkable success is shadowed by a critical challenge: their immense computational demands. The training and in- ference of state-of-the-art LLMs demand massive computational resources, leading to prohibitive expenses and significant environmental concerns (Strubell et al., 2019; Bender et al., 2021). At the heart of this inefficiency lies the foundational paradigm of these models: an autoregressive genera- tion process that operates on a sequence of discrete tokens. Because the computational cost scales with the length of the sequence, generating long-form text or processing extensive contexts remains a fundamental bottleneck, limiting the scalability and accessibility of these powerful models. The now-ubiquitous use of discrete tokens in LLMs is the result\n",
      "that operates on a sequence of discrete tokens. Because the computational cost scales with the length of the sequence, generating long-form text or processing extensive contexts remains a fundamental bottleneck, limiting the scalability and accessibility of these powerful models. The now-ubiquitous use of discrete tokens in LLMs is the result of a pivotal evolution from ear- lier modeling paradigms. Initially, models that operated at the character level struggled with the computational burden of extremely long sequences (Sutskever et al., 2011; Kim et al., 2016). The subsequent shift to modern subword tokenization (Sennrich et al., 2016) was driven by a crucial in- sight: increasing the information density of each text unit reduces sequence length and dramatically boosts model efficiency. This historical success suggests a clear path for unlocking the next order of magnitude in efficiency: continue to increase the semantic bandwidth of each predictive unit. We argue, however, that this path has reached a fundamental limit, constrained by the very nature of discrete representation. With typical vocabularies in modern LLMs ranging from approximately 32,000 to 256,000 entries, each token carries a surprisingly small amount of information—merely 15 to 18 bits (e.g.,log2(32768) = 15). To increase this capacity—for instance, to represent a\n",
      "has reached a fundamental limit, constrained by the very nature of discrete representation. With typical vocabularies in modern LLMs ranging from approximately 32,000 to 256,000 entries, each token carries a surprisingly small amount of information—merely 15 to 18 bits (e.g.,log2(32768) = 15). To increase this capacity—for instance, to represent a whole phrase—the vocabulary size would need to grow exponentially, making the final softmax computa- tion over this vocabulary an untenable bottleneck. This reveals a critical limitation: the information ∗Corresponding author. 1arXiv:2510.27688v1 [cs.CL] 31 Oct 2025Preprint Conventional LM: Next-Token PredictionThe cat sat on the mat Sequence Length = T CALM: Next-Vector PredictionVector 1 Vector 2 Sequence Length = T/KAutoencoder (K=3 tokens to 1 vector) Figure 1: Comparison between conventional token-by-token generation and our proposed vector-by- vector framework (CALM). By compressing K tokens into a single vector, we reduce the sequence length K-fold, fundamentally improving computational efficiency. density of discrete tokens is not scalable. Consequently, a profound mismatch has emerged: while model capacity has scaled to unprecedented levels, the task itself—predicting low-information dis- crete units one at a time—has not evolved. We are now deploying models of immense representa- tional power on a task that fundamentally limits their throughput, forcing them to\n"
     ]
    }
   ],
   "source": [
    "chunks = smart_chunk_text(doc, chunk_size=200, overlap=50)\n",
    "print(len(chunks))\n",
    "\n",
    "for i in range(4):\n",
    "    print(chunks[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d15fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "# embedder = SentenceTransformer(\"intfloat/multilingual-e5-large\")\n",
    "\n",
    "# llm = pipeline(\"text-generation\", model=\"gpt2\", max_new_tokens=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ff380da",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_MODEL = 'llama3:8b' \n",
    "\n",
    "\n",
    "def llama(prompt):\n",
    "    response = ollama.chat(\n",
    "        model=LLM_MODEL,\n",
    "        messages=[{'role': 'user', 'content': prompt}],\n",
    "    )\n",
    "\n",
    "    return response['message']['content']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73d6a2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(106, 384)\n"
     ]
    }
   ],
   "source": [
    "embeddings = embedder.encode(chunks, normalize_embeddings=True)\n",
    "print(embeddings.shape)\n",
    "\n",
    "dim = embeddings.shape[1]\n",
    "\n",
    "index = faiss.IndexHNSWFlat(dim, 32)\n",
    "index.hnsw.efSearch = 64\n",
    "index.add(embeddings)\n",
    "\n",
    "# index = faiss.IndexFlatIP(dim)\n",
    "# index.add(np.array(embeddings, dtype=np.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "799e003d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query, top_k=2):\n",
    "    query_emb = embedder.encode([query])\n",
    "    D, I = index.search(np.array(query_emb, dtype=np.float32), top_k)\n",
    "    print(\"retrieve \", D)\n",
    "    print()\n",
    "    return [chunks[i] for i in I[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9408f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_answer(query):\n",
    "    retrieved = retrieve(query)\n",
    "    context = \"\\n\".join(retrieved)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "        Ты — умный ассистент, который отвечает на вопросы, используя только предоставленный контекст.\n",
    "        Если ответ не содержится в тексте, напиши: \"Ответ не найден в документе.\"\n",
    "\n",
    "        Контекст:\n",
    "        {context}\n",
    "\n",
    "        Вопрос: {query}\n",
    "        \n",
    "    \"\"\"\n",
    "    # prompt = f\"Вопрос: {query}\\nОтвет:\"\n",
    "    \n",
    "    print('===== PROMT =====')\n",
    "    print(prompt)\n",
    "    print('=================')\n",
    "    print()\n",
    "\n",
    "    output = llama(prompt)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07195156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieve  [[0.95473516 0.9963665 ]]\n",
      "\n",
      "===== PROMT =====\n",
      "\n",
      "        Ты — умный ассистент, который отвечает на вопросы, используя только предоставленный контекст.\n",
      "        Если ответ не содержится в тексте, напиши: \"Ответ не найден в документе.\"\n",
      "\n",
      "        Контекст:\n",
      "        July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1355. URL https://aclanthology.org/P19-1355/. Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: En- hanced transformer with rotary position embedding.arXiv preprint arXiv:2104.09864, 2021. 27Preprint Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation, 2024a. URL https://arxiv.org/abs/2406.06525. Yutao Sun, Hangbo Bao, Wenhui Wang, Zhiliang Peng, Li Dong, Shaohan Huang, Jianyong Wang, and Furu Wei. Multimodal latent language modeling with next-token diffusion, 2024b. URL https://arxiv.org/abs/2412.08635. Ilya Sutskever, James Martens, and Geoffrey Hinton. Generating text with recurrent neural networks. In Lise Getoor and Tobias Scheffer (eds.),Proceedings of the 28th International Conference on Machine Learning (ICML-11), ICML ’11, pp. 1017–1024, New York, NY , USA, June 2011. ACM. ISBN 978-1-4503-0619-5. G´abor J Sz ´ekely. E-statistics: The energy of statistical samples.Bowling Green State University, Department of Mathematics and Statistics Technical Report, 3(05):1–18, 2003. LCM team, Lo ¨ıc Barrault, Paul-Ambroise Duquenne, Maha Elbayad, Artyom Kozhevnikov, Be- len Alastruey, Pierre Andrews, Mariano Coria, Guillaume Couairon, Marta R. Costa-juss `a, David Dale, Hady Elsahar, Kevin Heffernan, Jo ˜ao Maria Janeiro, Tuan Tran, Christophe Rop- ers, Eduardo S ´anchez, Robin San Roman, Alexandre\n",
      "Technical Report, 3(05):1–18, 2003. LCM team, Lo ¨ıc Barrault, Paul-Ambroise Duquenne, Maha Elbayad, Artyom Kozhevnikov, Be- len Alastruey, Pierre Andrews, Mariano Coria, Guillaume Couairon, Marta R. Costa-juss `a, David Dale, Hady Elsahar, Kevin Heffernan, Jo ˜ao Maria Janeiro, Tuan Tran, Christophe Rop- ers, Eduardo S ´anchez, Robin San Roman, Alexandre Mourachko, Safiyyah Saleem, and Holger Schwenk. Large concept models: Language modeling in a sentence representation space, 2024. URLhttps://arxiv.org/abs/2412.08821. NextStep Team. Nextstep-1: Toward autoregressive image generation with continuous tokens at scale, 2025. URLhttps://arxiv.org/abs/2508.10711. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth ´ee Lacroix, Baptiste Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models.arXiv preprint arXiv:2302.13971, 2023. Michael Tschannen, Cian Eastwood, and Fabian Mentzer. Givt: Generative infinite-vocabulary transformers.arXiv:2312.02116, 2023. Arnon Turetzky, Nimrod Shabtay, Slava Shechtman, Hagai Aronowitz, David Haws, Ron Hoory, and Avihu Dekel. Continuous speech synthesis using per-token latent diffusion, 2024. URL https://arxiv.org/abs/2410.16048. Amirhossein Vahidi, Simon Schosser, Lisa Wimmer, Yawei Li, Bernd Bischl, Eyke H ¨ullermeier, and Mina Rezaei. Probabilistic self-supervised representation learning via scoring rules minimization. InThe Twelfth International Conference on Learning Representations, 2024. URLhttps: //openreview.net/forum?id=skcTCdJz0f. Aaron van den Oord, Oriol Vinyals, and koray kavukcuoglu. Neural discrete representation learn-\n",
      "\n",
      "        Вопрос: when the article was published: \"Continuous Autoregressive Language Models?\"\n",
      "        Ответ:\n",
      "    \n",
      "=================\n",
      "\n",
      "===== ANSWER =====\n",
      "Ответ не найден в документе.\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "query = \"when the article was published: \\\"Continuous Autoregressive Language Models?\\\"\"\n",
    "answer = rag_answer(query)\n",
    "\n",
    "print('===== ANSWER =====')\n",
    "print(answer)\n",
    "print('==================')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
